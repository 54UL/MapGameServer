<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Cookbook &raquo; GPU Tasking (cudaFlow) | Taskflow QuickStart</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,400i,600,600i%7CSource+Code+Pro:400,400i,600" />
  <link rel="stylesheet" href="m-dark+documentation.compiled.css" />
  <link rel="icon" href="favicon.ico" type="image/x-icon" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#22272e" />
</head>
<body>
<header><nav id="navigation">
  <div class="m-container">
    <div class="m-row">
      <span id="m-navbar-brand" class="m-col-t-8 m-col-m-none m-left-m">
        <a href="https://taskflow.github.io"><img src="taskflow_logo.png" alt="" />Taskflow</a> <span class="m-breadcrumb">|</span> <a href="index.html" class="m-thin">QuickStart</a>
      </span>
      <div class="m-col-t-4 m-hide-m m-text-right m-nopadr">
        <a href="#search" class="m-doc-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
          <path id="m-doc-search-icon-path" d="m6 0c-3.31 0-6 2.69-6 6 0 3.31 2.69 6 6 6 1.49 0 2.85-0.541 3.89-1.44-0.0164 0.338 0.147 0.759 0.5 1.15l3.22 3.79c0.552 0.614 1.45 0.665 2 0.115 0.55-0.55 0.499-1.45-0.115-2l-3.79-3.22c-0.392-0.353-0.812-0.515-1.15-0.5 0.895-1.05 1.44-2.41 1.44-3.89 0-3.31-2.69-6-6-6zm0 1.56a4.44 4.44 0 0 1 4.44 4.44 4.44 4.44 0 0 1-4.44 4.44 4.44 4.44 0 0 1-4.44-4.44 4.44 4.44 0 0 1 4.44-4.44z"/>
        </svg></a>
        <a id="m-navbar-show" href="#navigation" title="Show navigation"></a>
        <a id="m-navbar-hide" href="#" title="Hide navigation"></a>
      </div>
      <div id="m-navbar-collapse" class="m-col-t-12 m-show-m m-col-m-none m-right-m">
        <div class="m-row">
          <ol class="m-col-t-6 m-col-m-none">
            <li><a href="pages.html">Handbook</a></li>
            <li><a href="namespaces.html">Namespaces</a></li>
          </ol>
          <ol class="m-col-t-6 m-col-m-none" start="3">
            <li><a href="annotated.html">Classes</a></li>
            <li><a href="files.html">Files</a></li>
            <li class="m-show-m"><a href="#search" class="m-doc-search-icon" title="Search" onclick="return showSearch()"><svg style="height: 0.9rem;" viewBox="0 0 16 16">
              <use href="#m-doc-search-icon-path" />
            </svg></a></li>
          </ol>
        </div>
      </div>
    </div>
  </div>
</nav></header>
<main><article>
  <div class="m-container m-container-inflatable">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <h1>
          <span class="m-breadcrumb"><a href="Cookbook.html">Cookbook</a> &raquo;</span>
          GPU Tasking (cudaFlow)
        </h1>
        <div class="m-block m-default">
          <h3>Contents</h3>
          <ul>
            <li><a href="#C6_Create_a_cudaFlow">Create a cudaFlow</a></li>
            <li><a href="#C6_Compile_a_cudaFlow_program">Compile a cudaFlow Program</a></li>
            <li><a href="#C6_run_a_cudaflow_on_multiple_gpus">Run a cudaFlow on Multiple GPUs</a></li>
            <li><a href="#C6_GPUMemoryOperations">Access GPU Memory</a></li>
            <li><a href="#C6_Granularity">Study the Granularity</a></li>
            <li><a href="#C6_OffloadAndUpdateAcudaFlow">Offload and Update a cudaFlow</a></li>
            <li><a href="#C6_UsecudaFlowInAStandaloneEnvironment">Use cudaFlow in a Standalone Environment</a></li>
          </ul>
        </div>
<p>Modern scientific computing typically leverages GPU-powered parallel processing cores to speed up large-scale applications. This chapter discusses how to implement CPU-GPU heterogeneous tasking algorithms with <a href="https://developer.nvidia.com/cuda-zone">Nvidia CUDA</a>.</p><section id="C6_Create_a_cudaFlow"><h2><a href="#C6_Create_a_cudaFlow">Create a cudaFlow</a></h2><p>Taskflow enables concurrent CPU-GPU tasking by leveraging <a href="https://developer.nvidia.com/blog/cuda-graphs/">CUDA Graph</a>. The tasking interface is referred to as <em>cudaFlow</em>. A cudaFlow is a graph object of type <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> created at runtime similar to dynamic tasking. It manages a task node in a taskflow and associates it with a CUDA Graph. To create a cudaFlow, emplace a callable with an argument of type <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a>. The following example implements the canonical saxpy (A·X Plus Y) task graph.</p><pre class="m-code"> <span class="mi">1</span><span class="o">:</span> <span class="err">#</span><span class="n">include</span> <span class="o">&lt;</span><span class="n">taskflow</span><span class="o">/</span><span class="n">cudaflow</span><span class="p">.</span><span class="n">hpp</span><span class="o">&gt;</span>
 <span class="mi">2</span><span class="o">:</span> 
 <span class="mi">3</span><span class="o">:</span> <span class="c1">// saxpy (single-precision A·X Plus Y) kernel</span>
 <span class="mi">4</span><span class="o">:</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">saxpy</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">5</span><span class="o">:</span>   <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
 <span class="mi">6</span><span class="o">:</span>   <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">7</span><span class="o">:</span>     <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
 <span class="mi">8</span><span class="o">:</span>   <span class="p">}</span>
 <span class="mi">9</span><span class="o">:</span> <span class="p">}</span>
<span class="mi">10</span><span class="o">:</span>
<span class="mi">11</span><span class="o">:</span> <span class="c1">// main function begins</span>
<span class="mi">12</span><span class="o">:</span> <span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
<span class="mi">13</span><span class="o">:</span>
<span class="mi">14</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Taskflow</span> <span class="n">taskflow</span><span class="p">;</span>
<span class="mi">15</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Executor</span> <span class="n">executor</span><span class="p">;</span>
<span class="mi">16</span><span class="o">:</span>  
<span class="mi">17</span><span class="o">:</span>   <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span>                            <span class="c1">// size of the vector</span>
<span class="mi">18</span><span class="o">:</span>
<span class="mi">19</span><span class="o">:</span>   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">hx</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">1.0f</span><span class="p">);</span>                      <span class="c1">// x vector at host</span>
<span class="mi">20</span><span class="o">:</span>   <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">hy</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">);</span>                      <span class="c1">// y vector at host</span>
<span class="mi">21</span><span class="o">:</span>
<span class="mi">22</span><span class="o">:</span>   <span class="kt">float</span> <span class="o">*</span><span class="n">dx</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>                                  <span class="c1">// x vector at device</span>
<span class="mi">23</span><span class="o">:</span>   <span class="kt">float</span> <span class="o">*</span><span class="n">dy</span><span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>                                  <span class="c1">// y vector at device</span>
<span class="mi">24</span><span class="o">:</span>  
<span class="mi">25</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">allocate_x</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span>
<span class="mi">26</span><span class="o">:</span>     <span class="p">[</span><span class="o">&amp;</span><span class="p">](){</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));}</span>
<span class="mi">27</span><span class="o">:</span>   <span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;allocate_x&quot;</span><span class="p">);</span>
<span class="mi">28</span><span class="o">:</span>
<span class="mi">29</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">allocate_y</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span>
<span class="mi">30</span><span class="o">:</span>     <span class="p">[</span><span class="o">&amp;</span><span class="p">](){</span> <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));}</span>
<span class="mi">31</span><span class="o">:</span>   <span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;allocate_y&quot;</span><span class="p">);</span>
<span class="mi">32</span><span class="o">:</span>
<span class="mi">33</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaflow</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">34</span><span class="o">:</span>     <span class="c1">// create data transfer tasks</span>
<span class="mi">35</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span> 
<span class="mi">36</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
<span class="mi">37</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
<span class="mi">38</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
<span class="mi">39</span><span class="o">:</span>
<span class="mi">40</span><span class="o">:</span>     <span class="c1">// launch saxpy&lt;&lt;&lt;(N+255)/256, 256, 0&gt;&gt;&gt;(N, 2.0f, dx, dy)</span>
<span class="mi">41</span><span class="o">:</span>     <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span>
<span class="mi">42</span><span class="o">:</span>       <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span>
<span class="mi">43</span><span class="o">:</span>     <span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
<span class="mi">44</span><span class="o">:</span>
<span class="mi">45</span><span class="o">:</span>     <span class="n">kernel</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>
<span class="mi">46</span><span class="o">:</span>           <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span>
<span class="mi">48</span><span class="o">:</span>   <span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
<span class="mi">49</span><span class="o">:</span>   <span class="n">cudaflow</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">allocate_x</span><span class="p">,</span> <span class="n">allocate_y</span><span class="p">);</span>  <span class="c1">// overlap memory alloc</span>
<span class="mi">50</span><span class="o">:</span>  
<span class="mi">51</span><span class="o">:</span>   <span class="n">executor</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">taskflow</span><span class="p">).</span><span class="n">wait</span><span class="p">();</span>
<span class="mi">52</span><span class="o">:</span>
<span class="mi">53</span><span class="o">:</span>   <span class="n">taskflow</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="p">);</span>                  <span class="c1">// dump the taskflow</span>
<span class="mi">54</span><span class="o">:</span> <span class="p">}</span></pre><div class="m-graph"><svg style="width: 30.000rem; height: 14.562rem;" viewBox="0.00 0.00 479.55 232.77">
<g transform="scale(1 1) rotate(0) translate(4 228.77)">
<title>Taskflow</title>
<g class="m-cluster">
<title>cluster_p0x55b2191178a8</title>
<polygon points="8,-47.38 8,-180.38 463.55,-180.38 463.55,-47.38 8,-47.38"/>
<text text-anchor="middle" x="235.77" y="-163.58">cudaFlow: saxpy</text>
</g>
<g class="m-node m-flat">
<title>p0x55b219117698</title>
<ellipse cx="296.91" cy="-206.38" rx="63.78" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-202.58">allocate_x</text>
</g>
<g class="m-node">
<title>p0x55b2191178a8</title>
<polygon points="455.55,-118.38 452.55,-122.38 431.55,-122.38 428.55,-118.38 396.55,-118.38 396.55,-82.38 455.55,-82.38 455.55,-118.38"/>
<text text-anchor="middle" x="426.05" y="-96.58">saxpy</text>
</g>
<g class="m-edge">
<title>p0x55b219117698&#45;&gt;p0x55b2191178a8</title>
<path d="M343.62,-193.87C349.59,-191.24 355.41,-188.1 360.55,-184.38 381.31,-169.35 398.59,-145.77 410.02,-127.42"/>
<polygon points="413.15,-129 415.3,-118.63 407.15,-125.4 413.15,-129"/>
</g>
<g class="m-node m-flat">
<title>p0x55b2191177a0</title>
<ellipse cx="296.91" cy="-18.38" rx="63.78" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-14.58">allocate_y</text>
</g>
<g class="m-edge">
<title>p0x55b2191177a0&#45;&gt;p0x55b2191178a8</title>
<path d="M338.66,-32.28C346.17,-35.49 353.77,-39.21 360.55,-43.38 374.92,-52.23 389.16,-64.37 400.6,-75.17"/>
<polygon points="398.41,-77.92 408.03,-82.37 403.29,-72.9 398.41,-77.92"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401a50</title>
<ellipse cx="59.13" cy="-128.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="59.13" y="-124.58">h2d_x</text>
</g>
<g class="m-node">
<title>p0x7f2870402bc0</title>
<polygon points="197.27,-118.38 142.27,-118.38 138.27,-114.38 138.27,-82.38 193.27,-82.38 197.27,-86.38 197.27,-118.38"/>
<polyline points="193.27,-114.38 138.27,-114.38 "/>
<polyline points="193.27,-114.38 193.27,-82.38 "/>
<polyline points="193.27,-114.38 197.27,-118.38 "/>
<text text-anchor="middle" x="167.77" y="-96.58">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7f2870401a50&#45;&gt;p0x7f2870402bc0</title>
<path d="M96.23,-118.91C106.58,-116.19 117.9,-113.22 128.38,-110.46"/>
<polygon points="129.33,-113.83 138.11,-107.91 127.55,-107.06 129.33,-113.83"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402310</title>
<ellipse cx="296.91" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-69.58">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402310</title>
<path d="M197.3,-94.33C212.23,-91.15 230.92,-87.19 247.99,-83.56"/>
<polygon points="248.77,-86.97 257.82,-81.47 247.31,-80.13 248.77,-86.97"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402780</title>
<ellipse cx="296.91" cy="-128.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="296.91" y="-124.58">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402780</title>
<path d="M197.3,-106.67C212.36,-109.98 231.22,-114.14 248.41,-117.93"/>
<polygon points="247.78,-121.37 258.3,-120.1 249.29,-114.53 247.78,-121.37"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401eb0</title>
<ellipse cx="59.13" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="59.13" y="-69.58">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870401eb0&#45;&gt;p0x7f2870402bc0</title>
<path d="M96.53,-82.6C106.67,-85.17 117.71,-87.96 127.98,-90.56"/>
<polygon points="127.37,-94.02 137.93,-93.08 129.09,-87.24 127.37,-94.02"/>
</g>
<g class="m-edge">
<title>p0x7f2870402310&#45;&gt;p0x55b2191178a8</title>
<path d="M336.05,-81.49C351.89,-84.85 370.25,-88.75 386.17,-92.13"/>
<polygon points="385.72,-95.61 396.23,-94.26 387.17,-88.76 385.72,-95.61"/>
</g>
<g class="m-edge">
<title>p0x7f2870402780&#45;&gt;p0x55b2191178a8</title>
<path d="M335.71,-120.06C351.68,-116.54 370.26,-112.45 386.33,-108.91"/>
<polygon points="387.46,-112.25 396.48,-106.68 385.96,-105.41 387.46,-112.25"/>
</g>
</g>
</svg>
</div><p>Debrief:</p><ul><li>Lines 3-9 define a saxpy kernel using CUDA</li><li>Lines 19-20 declare two host vectors, <code>hx</code> and <code>hy</code></li><li>Lines 22-23 declare two device vector pointers, <code>dx</code> and <code>dy</code></li><li>Lines 25-31 declare two tasks to allocate memory for <code>dx</code> and <code>dy</code> on device, each of <code>N*sizeof(float)</code> bytes</li><li>Lines 33-48 create a cudaFlow to define a GPU task graph (two host-to-device data transfer tasks, one saxpy kernel task, and two device-to-host data transfer tasks)</li><li>Lines 49-53 define the task dependency between host tasks and the cudaFlow tasks and execute the taskflow</li></ul><p>Taskflow does not expend unnecessary efforts on kernel programming but focus on tasking CUDA operations with CPU work. We give users full privileges to craft a CUDA kernel that is commensurate with their domain knowledge. Users focus on developing high-performance kernels using a native CUDA toolkit, while leaving difficult task parallelism to Taskflow.</p><aside class="m-note m-warning"><h4>Attention</h4><p>You need to include <code><a href="cudaflow_8hpp.html" class="m-doc">taskflow/<wbr />cudaflow.hpp</a></code> in order to use <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a>.</p></aside></section><section id="C6_Compile_a_cudaFlow_program"><h2><a href="#C6_Compile_a_cudaFlow_program">Compile a cudaFlow Program</a></h2><p>Use <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">nvcc</a> (at least v11.1) to compile a cudaFlow program:</p><pre class="m-console"><span class="go">~$ nvcc -std=c++17 my_cudaflow.cu -I path/to/include/taskflow -O2 -o my_cudaflow</span>
<span class="go">~$ ./my_cudaflow</span></pre><p>Please visit the page <a href="CompileTaskflowWithCUDA.html" class="m-doc">Compile Taskflow with CUDA</a> for more details.</p></section><section id="C6_run_a_cudaflow_on_multiple_gpus"><h2><a href="#C6_run_a_cudaflow_on_multiple_gpus">Run a cudaFlow on Multiple GPUs</a></h2><p>By default, a cudaFlow runs on the current GPU associated with the caller, which is typically <code>0</code>. You can run a cudaFlow on multiple GPUs by explicitly associating a cudaFlow or a kernel task with a CUDA device. A CUDA device is an integer number in the range of <code>[0, N)</code> representing the identifier of a GPU, where <code>N</code> is the number of GPUs in a system. The code below creates a cudaFlow that runs on GPU <code>0</code>.</p><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([]</span> <span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{},</span> <span class="mi">0</span><span class="p">);</span>  <span class="c1">// place the cudaFlow on GPU 0</span></pre><p>You can place a kernel on a GPU explicitly through the method <a href="classtf_1_1cudaFlow.html#a4a839dbaa01237a440edfebe8faf4e5b" class="m-doc">tf::<wbr />cudaFlow::<wbr />kernel_on</a> that takes the GPU device identifier in the first argument.</p><pre class="m-code"> <span class="mi">1</span><span class="o">:</span> <span class="err">#</span><span class="n">include</span> <span class="o">&lt;</span><span class="n">taskflow</span><span class="o">/</span><span class="n">cudaflow</span><span class="p">.</span><span class="n">hpp</span><span class="o">&gt;</span>
 <span class="mi">2</span><span class="o">:</span> 
 <span class="mi">3</span><span class="o">:</span> <span class="c1">// saxpy (single-precision A·X Plus Y) kernel</span>
 <span class="mi">4</span><span class="o">:</span> <span class="n">__global__</span> <span class="kt">void</span> <span class="n">saxpy</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">float</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">y</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">z</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">5</span><span class="o">:</span>  <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
 <span class="mi">6</span><span class="o">:</span>  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
 <span class="mi">7</span><span class="o">:</span>    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
 <span class="mi">8</span><span class="o">:</span>   <span class="p">}</span>
 <span class="mi">9</span><span class="o">:</span> <span class="p">}</span>
<span class="mi">10</span><span class="o">:</span>
<span class="mi">11</span><span class="o">:</span> <span class="kt">int</span> <span class="n">main</span><span class="p">()</span> <span class="p">{</span>
<span class="mi">12</span><span class="o">:</span>
<span class="mi">13</span><span class="o">:</span>   <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">N</span> <span class="o">=</span> <span class="mi">1</span><span class="o">&lt;&lt;</span><span class="mi">20</span><span class="p">;</span>
<span class="mi">14</span><span class="o">:</span>   
<span class="mi">15</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">dx</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">16</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">dy</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">17</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">z1</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">18</span><span class="o">:</span>   <span class="kt">float</span><span class="o">*</span> <span class="n">z2</span> <span class="p">{</span><span class="k">nullptr</span><span class="p">};</span>
<span class="mi">19</span><span class="o">:</span>  
<span class="mi">20</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// create unified memory for x</span>
<span class="mi">21</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// create unified memory for y</span>
<span class="mi">22</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">z1</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// result of saxpy task 1</span>
<span class="mi">23</span><span class="o">:</span>   <span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">z2</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>  <span class="c1">// result of saxpy task 2</span>
<span class="mi">24</span><span class="o">:</span>  
<span class="mi">25</span><span class="o">:</span>   <span class="k">for</span><span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">26</span><span class="o">:</span>     <span class="n">dx</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="mi">27</span><span class="o">:</span>     <span class="n">dy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="mi">28</span><span class="o">:</span>   <span class="p">}</span>
<span class="mi">29</span><span class="o">:</span>
<span class="mi">30</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Taskflow</span> <span class="n">taskflow</span><span class="p">;</span>
<span class="mi">31</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">Executor</span> <span class="n">executor</span><span class="p">;</span>
<span class="mi">32</span><span class="o">:</span>  
<span class="mi">33</span><span class="o">:</span>   <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
<span class="mi">34</span><span class="o">:</span>     <span class="c1">// We create a cudaFlow on GPU 0. The scheduler will switch to </span>
<span class="mi">35</span><span class="o">:</span>     <span class="c1">// GPU context 0 when running this callable.</span>
<span class="mi">36</span><span class="o">:</span>
<span class="mi">37</span><span class="o">:</span>     <span class="c1">// launch the first saxpy kernel on GPU 1</span>
<span class="mi">38</span><span class="o">:</span>     <span class="n">cf</span><span class="p">.</span><span class="n">kernel_on</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z1</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;1&quot;</span><span class="p">);</span>
<span class="mi">39</span><span class="o">:</span>
<span class="mi">40</span><span class="o">:</span>     <span class="c1">// launch the second saxpy kernel on GPU 3</span>
<span class="mi">41</span><span class="o">:</span>     <span class="n">cf</span><span class="p">.</span><span class="n">kernel_on</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z2</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;3&quot;</span><span class="p">);</span>
<span class="mi">42</span><span class="o">:</span>   <span class="p">},</span> <span class="mi">0</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;cudaFlow on GPU 0&quot;</span><span class="p">);</span>
<span class="mi">43</span><span class="o">:</span>
<span class="mi">44</span><span class="o">:</span>   <span class="n">executor</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">taskflow</span><span class="p">).</span><span class="n">wait</span><span class="p">();</span>
<span class="mi">45</span><span class="o">:</span>
<span class="mi">46</span><span class="o">:</span>   <span class="n">cudaFree</span><span class="p">(</span><span class="n">dx</span><span class="p">);</span>
<span class="mi">47</span><span class="o">:</span>   <span class="n">cudaFree</span><span class="p">(</span><span class="n">dy</span><span class="p">);</span>
<span class="mi">48</span><span class="o">:</span>  
<span class="mi">49</span><span class="o">:</span>   <span class="c1">// verify the solution; max_error should be zero</span>
<span class="mi">50</span><span class="o">:</span>   <span class="kt">float</span> <span class="n">max_error</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="mi">51</span><span class="o">:</span>   <span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">52</span><span class="o">:</span>     <span class="n">max_error</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_error</span><span class="p">,</span> <span class="n">abs</span><span class="p">(</span><span class="n">z1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="mi">-4</span><span class="p">));</span>
<span class="mi">53</span><span class="o">:</span>     <span class="n">max_error</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">max_error</span><span class="p">,</span> <span class="n">abs</span><span class="p">(</span><span class="n">z2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="mi">-4</span><span class="p">));</span>
<span class="mi">54</span><span class="o">:</span>   <span class="p">}</span>
<span class="mi">55</span><span class="o">:</span>   <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">&quot;saxpy finished with max error: &quot;</span> <span class="o">&lt;&lt;</span> <span class="n">max_error</span> <span class="o">&lt;&lt;</span> <span class="sc">&#39;\n&#39;</span><span class="p">;</span>
<span class="mi">56</span><span class="o">:</span> <span class="p">}</span></pre><p>Debrief:</p><ul><li>Lines 3-9 define a CUDA saxpy kernel that stores the result to <code>z</code></li><li>Lines 15-23 declare four unified memory blocks accessible from any processor</li><li>Lines 25-28 initialize <code>dx</code> and <code>dy</code> blocks by CPU</li><li>Lines 33-42 create a cudaFlow task on GPU <code>0</code> using <a href="classtf_1_1FlowBuilder.html#afdf47fd1a358fb64f8c1b89e2a393169" class="m-doc">tf::<wbr />Taskflow::<wbr />emplace_on</a></li><li>Lines 37-38 create a kernel task to launch the first saxpy on GPU <code>1</code> and store the result in <code>z1</code></li><li>Lines 40-41 create a kernel task to launch the second saxpy on GPU <code>3</code> and store the result in <code>z2</code></li><li>Lines 44-55 run the taskflow and verify the result (<code>max_error</code> should be zero)</li></ul><p>Running the program gives the following <a href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html">nvidia-smi</a> snapshot in a system of 4 GPUs:</p><pre class="m-console"><span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                       GPU Memory |</span>
<span class="go">|  GPU       PID   Type   Process name                             Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|    0     53869      C   ./a.out                                      153MiB |</span>
<span class="go">|    1     53869      C   ./a.out                                      155MiB |</span>
<span class="go">|    3     53869      C   ./a.out                                      155MiB |</span>
<span class="go">+-----------------------------------------------------------------------------+</span></pre><aside class="m-note m-warning"><h4>Attention</h4><p><a href="classtf_1_1FlowBuilder.html#afdf47fd1a358fb64f8c1b89e2a393169" class="m-doc">tf::<wbr />Taskflow::<wbr />emplace_on</a> allows you to place a cudaFlow on a particular GPU device, but it is your responsibility to ensure correct memory access. For example, you may not allocate a memory block on GPU <code>2</code> using <code>cudaMalloc</code> and access it from a kernel on GPU <code>1</code>.</p></aside><p>An easy practice is to allocate <em>unified shared memory</em> using <code>cudaMallocManaged</code> and let the CUDA runtime perform automatic memory migration between processors (as demonstrated in the code example above).</p><p>As the same example, you may create two cudaFlows for the two kernels on two GPUs, respectively. The overhead of creating a kernel on the same device as a cudaFlow is much less than the different one.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaFlow_on_gpu1</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z1</span><span class="p">);</span>
<span class="p">},</span> <span class="mi">1</span><span class="p">);</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaFlow_on_gpu3</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace_on</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">z2</span><span class="p">);</span>
<span class="p">},</span> <span class="mi">3</span><span class="p">);</span></pre></section><section id="C6_GPUMemoryOperations"><h2><a href="#C6_GPUMemoryOperations">Access GPU Memory</a></h2><p><a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> provides a set of methods for users to manipulate device memory data. There are two categories, raw data and typed data. Raw data operations are methods with prefix <code>mem</code>, such as <code>memcpy</code> and <code>memset</code>, that take action on GPU memory area in <em>bytes</em>. Typed data operations such as <code>copy</code>, <code>fill</code>, and <code>zero</code>, take <em>logical count</em> of elements. For instance, the following three methods have the same result of zeroing <code>sizeof(int)*count</code> bytes of the device memory area pointed to by <code>target</code>.</p><pre class="m-code"><span class="kt">int</span><span class="o">*</span> <span class="n">target</span><span class="p">;</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">target</span><span class="p">,</span> <span class="n">count</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span>

<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">memset_target</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">memset</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">count</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">same_as_above</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">same_as_above_again</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">zero</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
<span class="p">});</span></pre><p>The method <a href="classtf_1_1cudaFlow.html#a21d4447bc834f4d3e1bb4772c850d090" class="m-doc">cudaFlow::<wbr />fill</a> is a more powerful version of <a href="classtf_1_1cudaFlow.html#a079ca65da35301e5aafd45878a19e9d2" class="m-doc">cudaFlow::<wbr />memset</a>. It can fill a memory area with any value of type <code>T</code>, given that <code>sizeof(T)</code> is 1, 2, or 4 bytes. For example, the following code sets each element in the array <code>target</code> to 1234.</p><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span> <span class="n">cf</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span> <span class="p">});</span></pre><p>Similar concept applies to <a href="classtf_1_1cudaFlow.html#ad37637606f0643f360e9eda1f9a6e559" class="m-doc">cudaFlow::<wbr />memcpy</a> and <a href="classtf_1_1cudaFlow.html#af03e04771b655f9e629eb4c22e19b19f" class="m-doc">cudaFlow::<wbr />copy</a> as well.</p><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">){</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">memcpy_target</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">memcpy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="o">*</span> <span class="n">count</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">same_as_above</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">count</span><span class="p">);</span>
<span class="p">});</span></pre></section><section id="C6_Granularity"><h2><a href="#C6_Granularity">Study the Granularity</a></h2><p>Creating a cudaFlow has certain overhead, which means fined-grained tasking such as one GPU operation per cudaFlow may not give you any performance gain. You should aggregate as many GPU operations as possible in a cudaFlow to launch the entire graph once instead of separate calls. For example, the following code creates the saxpy task graph at a very fine-grained level using one cudaFlow per GPU operation.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>  <span class="c1">// creates the 1st cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>  <span class="c1">// creates the 2nd cudaFlow </span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>  <span class="c1">// creates the 3rd cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>  <span class="c1">// creates the 4th cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">kernel</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;kernel&quot;</span><span class="p">);</span> <span class="c1">// creates the 5th cudaFlow</span>

<span class="n">kernel</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>
      <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span></pre><div class="m-graph"><svg style="width: 37.750rem; height: 21.250rem;" viewBox="0.00 0.00 604.00 339.54">
<g transform="scale(1 1) rotate(0) translate(4 335.54)">
<title>Taskflow</title>
<g class="m-cluster">
<title>cluster_p0x21987b0</title>
<polygon points="449,-166.77 449,-323.54 588,-323.54 588,-166.77 449,-166.77"/>
<text text-anchor="middle" x="518.5" y="-306.74">cudaFlow: h2d_x</text>
</g>
<g class="m-cluster">
<title>cluster_p0x2198870</title>
<polygon points="302,-166.77 302,-323.54 441,-323.54 441,-166.77 302,-166.77"/>
<text text-anchor="middle" x="371.5" y="-306.74">cudaFlow: h2d_y</text>
</g>
<g class="m-cluster">
<title>cluster_p0x2198930</title>
<polygon points="8,-8 8,-158.77 147,-158.77 147,-8 8,-8"/>
<text text-anchor="middle" x="77.5" y="-141.97">cudaFlow: d2h_x</text>
</g>
<g class="m-cluster">
<title>cluster_p0x21989f0</title>
<polygon points="302,-8 302,-158.77 441,-158.77 441,-8 302,-8"/>
<text text-anchor="middle" x="371.5" y="-141.97">cudaFlow: d2h_y</text>
</g>
<g class="m-cluster">
<title>cluster_p0x2198ab0</title>
<polygon points="155,-80.38 155,-244.77 294,-244.77 294,-80.38 155,-80.38"/>
<text text-anchor="middle" x="224.5" y="-227.97">cudaFlow: kernel</text>
</g>
<g class="m-node">
<title>p0x21987b0</title>
<polygon points="524.5,-210.77 521.5,-214.77 500.5,-214.77 497.5,-210.77 463.5,-210.77 463.5,-174.77 524.5,-174.77 524.5,-210.77"/>
<text text-anchor="middle" x="494" y="-188.97">h2d_x</text>
</g>
<g class="m-node">
<title>p0x2198ab0</title>
<polygon points="285.5,-124.38 282.5,-128.38 261.5,-128.38 258.5,-124.38 224.5,-124.38 224.5,-88.38 285.5,-88.38 285.5,-124.38"/>
<text text-anchor="middle" x="255" y="-102.58">kernel</text>
</g>
<g class="m-edge">
<title>p0x21987b0&#45;&gt;p0x2198ab0</title>
<path d="M465.52,-174.74C458.99,-171.55 451.92,-168.63 445,-166.77 413.41,-158.26 327.25,-173.42 298,-158.77 286.58,-153.05 277.04,-142.78 269.87,-132.92"/>
<polygon points="272.68,-130.82 264.2,-124.48 266.88,-134.73 272.68,-130.82"/>
</g>
<g class="m-node">
<title>p0x2198930</title>
<polygon points="132.5,-52 129.5,-56 108.5,-56 105.5,-52 71.5,-52 71.5,-16 132.5,-16 132.5,-52"/>
<text text-anchor="middle" x="102" y="-30.2">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x2198ab0&#45;&gt;p0x2198930</title>
<path d="M224.41,-91.31C200.64,-80.38 167.49,-65.13 141.69,-53.26"/>
<polygon points="143.09,-50.05 132.54,-49.05 140.16,-56.41 143.09,-50.05"/>
</g>
<g class="m-node">
<title>p0x21989f0</title>
<polygon points="377.5,-52 374.5,-56 353.5,-56 350.5,-52 316.5,-52 316.5,-16 377.5,-16 377.5,-52"/>
<text text-anchor="middle" x="347" y="-30.2">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x2198ab0&#45;&gt;p0x21989f0</title>
<path d="M277.27,-88.35C289.05,-79.34 303.69,-68.14 316.5,-58.33"/>
<polygon points="318.8,-60.98 324.62,-52.13 314.55,-55.42 318.8,-60.98"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe390000e60</title>
<ellipse cx="500" cy="-271.15" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="500" y="-267.35">h2d_x</text>
</g>
<g class="m-edge">
<title>p0x7fe390000e60&#45;&gt;p0x21987b0</title>
<path d="M498.61,-252.44C497.88,-243.15 496.97,-231.56 496.15,-221.15"/>
<polygon points="499.62,-220.56 495.34,-210.86 492.64,-221.11 499.62,-220.56"/>
</g>
<g class="m-node">
<title>p0x2198870</title>
<polygon points="377.5,-210.77 374.5,-214.77 353.5,-214.77 350.5,-210.77 316.5,-210.77 316.5,-174.77 377.5,-174.77 377.5,-210.77"/>
<text text-anchor="middle" x="347" y="-188.97">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x2198870&#45;&gt;p0x2198ab0</title>
<path d="M318.78,-174.73C311.7,-169.94 304.32,-164.47 298,-158.77 289.36,-150.98 280.9,-141.43 273.81,-132.69"/>
<polygon points="276.4,-130.33 267.46,-124.64 270.9,-134.66 276.4,-130.33"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe390001890</title>
<ellipse cx="353" cy="-271.15" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="353" y="-267.35">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7fe390001890&#45;&gt;p0x2198870</title>
<path d="M351.61,-252.44C350.88,-243.15 349.97,-231.56 349.15,-221.15"/>
<polygon points="352.62,-220.56 348.34,-210.86 345.64,-221.11 352.62,-220.56"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe39000b790</title>
<ellipse cx="96" cy="-106.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="96" y="-102.58">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7fe39000b790&#45;&gt;p0x2198930</title>
<path d="M97.48,-87.99C98.14,-80.23 98.94,-70.91 99.68,-62.26"/>
<polygon points="103.17,-62.46 100.53,-52.2 96.2,-61.86 103.17,-62.46"/>
</g>
<g class="m-node m-flat">
<title>p0x7fe3900017e0</title>
<ellipse cx="353" cy="-106.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="353" y="-102.58">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7fe3900017e0&#45;&gt;p0x21989f0</title>
<path d="M351.52,-87.99C350.86,-80.23 350.06,-70.91 349.32,-62.26"/>
<polygon points="352.8,-61.86 348.47,-52.2 345.83,-62.46 352.8,-61.86"/>
</g>
<g class="m-node">
<title>p0x7fe390002000</title>
<polygon points="284.5,-210.77 229.5,-210.77 225.5,-206.77 225.5,-174.77 280.5,-174.77 284.5,-178.77 284.5,-210.77"/>
<polyline points="280.5,-206.77 225.5,-206.77 "/>
<polyline points="280.5,-206.77 280.5,-174.77 "/>
<polyline points="280.5,-206.77 284.5,-210.77 "/>
<text text-anchor="middle" x="255" y="-188.97">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7fe390002000&#45;&gt;p0x2198ab0</title>
<path d="M255,-174.69C255,-163.26 255,-147.95 255,-134.82"/>
<polygon points="258.5,-134.44 255,-124.44 251.5,-134.44 258.5,-134.44"/>
</g>
</g>
</svg>
</div><p>The following code aggregates the five GPU operations using one cudaFlow and achieves better performance.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">Task</span> <span class="n">cudaflow</span> <span class="o">=</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
  <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">saxpy</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
                         <span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>
  <span class="n">saxpy</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>
       <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span>
<span class="p">}).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>  <span class="c1">// creates one cudaFlow</span></pre><div class="m-graph"><svg style="width: 19.500rem; height: 6.250rem;" viewBox="0.00 0.00 311.53 99.77">
<g transform="scale(1 1) rotate(0) translate(4 95.77)">
<title>Taskflow</title>
<g class="m-node m-flat">
<title>p0x7f2870401a50</title>
<ellipse cx="43.13" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="43.13" y="-69.58">h2d_x</text>
</g>
<g class="m-node">
<title>p0x7f2870402bc0</title>
<polygon points="181.27,-63.38 126.27,-63.38 122.27,-59.38 122.27,-27.38 177.27,-27.38 181.27,-31.38 181.27,-63.38"/>
<polyline points="177.27,-59.38 122.27,-59.38 "/>
<polyline points="177.27,-59.38 177.27,-27.38 "/>
<polyline points="177.27,-59.38 181.27,-63.38 "/>
<text text-anchor="middle" x="151.77" y="-41.58">saxpy</text>
</g>
<g class="m-edge">
<title>p0x7f2870401a50&#45;&gt;p0x7f2870402bc0</title>
<path d="M80.23,-63.91C90.58,-61.19 101.9,-58.22 112.38,-55.46"/>
<polygon points="113.33,-58.83 122.11,-52.91 111.55,-52.06 113.33,-58.83"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402310</title>
<ellipse cx="260.4" cy="-73.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="260.4" y="-69.58">d2h_x</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402310</title>
<path d="M181.34,-52.89C191.17,-55.47 202.43,-58.43 213.29,-61.28"/>
<polygon points="212.53,-64.7 223.09,-63.85 214.31,-57.93 212.53,-64.7"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870402780</title>
<ellipse cx="260.4" cy="-18.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="260.4" y="-14.58">d2h_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870402bc0&#45;&gt;p0x7f2870402780</title>
<path d="M181.34,-38.15C191.07,-35.69 202.22,-32.86 212.98,-30.14"/>
<polygon points="213.87,-33.52 222.7,-27.68 212.15,-26.74 213.87,-33.52"/>
</g>
<g class="m-node m-flat">
<title>p0x7f2870401eb0</title>
<ellipse cx="43.13" cy="-18.38" rx="43.27" ry="18.27"/>
<text text-anchor="middle" x="43.13" y="-14.58">h2d_y</text>
</g>
<g class="m-edge">
<title>p0x7f2870401eb0&#45;&gt;p0x7f2870402bc0</title>
<path d="M80.53,-27.6C90.67,-30.17 101.71,-32.96 111.98,-35.56"/>
<polygon points="111.37,-39.02 121.93,-38.08 113.09,-32.24 111.37,-39.02"/>
</g>
</g>
</svg>
</div><aside class="m-note m-info"><h4>Note</h4><p>We encourage users to study and understand the parallel structure of their applications, in order to come up with the best granularity of task decomposition. A refined task graph can have significant performance difference from the raw counterpart.</p></aside></section><section id="C6_OffloadAndUpdateAcudaFlow"><h2><a href="#C6_OffloadAndUpdateAcudaFlow">Offload and Update a cudaFlow</a></h2><p>Many GPU applications require you to launch a cudaFlow mutiple times and update node parameters (e.g., kernel parameters and memory addresses) between iterations. <a href="classtf_1_1cudaFlow.html#a85789ed8a1f47704cf1f1a2b98969444" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload</a> allows you to execute the graph immediately and then update the parameters for the next execution. When you offload a cudaFlow, an executable graph will be created, and you must NOT change the topology but the node parameters between successive executions.</p><pre class="m-code"><span class="mi">1</span><span class="o">:</span> <span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span> <span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
<span class="mi">2</span><span class="o">:</span>   <span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">task</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid1</span><span class="p">,</span> <span class="n">block1</span><span class="p">,</span> <span class="n">shm1</span><span class="p">,</span> <span class="n">my_kernel</span><span class="p">,</span> <span class="n">args1</span><span class="p">...);</span>
<span class="mi">3</span><span class="o">:</span>   <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>  <span class="c1">// immediately run the cudaFlow once</span>
<span class="mi">4</span><span class="o">:</span>
<span class="mi">5</span><span class="o">:</span>   <span class="n">cf</span><span class="p">.</span><span class="n">update_kernel</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">grid2</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">shm2</span><span class="p">,</span> <span class="n">args2</span><span class="p">...);</span>
<span class="mi">6</span><span class="o">:</span>   <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>  <span class="c1">// run the cudaFlow again with the same graph topology</span>
<span class="mi">7</span><span class="o">:</span>                  <span class="c1">// but with different kernel parameters</span>
<span class="mi">8</span><span class="o">:</span> <span class="p">});</span></pre><p>Line 2 creates a kernel task to run <code>my_kernel</code> with the given parameters. Line 3 offloads the cudaFlow and performs an immediate execution; afterwards, we must not modify the graph topology. Line 5 updates the parameters of <code>my_kernel</code> associated with <code>task</code>. Line 6 executes the cudaFlow again with updated kernel parameters. We currently supports the following offload methods:</p><ul><li><a href="classtf_1_1cudaFlow.html#a85789ed8a1f47704cf1f1a2b98969444" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload</a> offloads and runs the cudaFlow once</li><li><a href="classtf_1_1cudaFlow.html#ac2269fd7dc8ca04a294a718204703dad" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload_n</a> offloads and runs the cudaFlow <code>n</code> times</li><li><a href="classtf_1_1cudaFlow.html#a99358da15e3bdfa1faabb3e326130e1f" class="m-doc">tf::<wbr />cudaFlow::<wbr />offload_until</a> offloads and keeps running the cudaFlow until the given predicate returns <code>true</code></li></ul><pre class="m-code"><span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// ... create CUDA tasks</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>      <span class="c1">// offload the cudaFlow and run it once</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload_n</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>  <span class="c1">// offload the cudaFlow and run it 10 times</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload_until</span><span class="p">([</span><span class="n">repeat</span><span class="o">=</span><span class="mi">5</span><span class="p">]</span> <span class="p">()</span> <span class="k">mutable</span> <span class="p">{</span> <span class="k">return</span> <span class="n">repeat</span><span class="o">--</span> <span class="o">==</span> <span class="mi">0</span><span class="p">;</span> <span class="p">})</span>  <span class="c1">// 5 times</span>
<span class="p">};</span></pre><p>After you offload a cudaFlow (possibly multiple times), it is considered executed, and the executor will not run an offloaded cudaFlow after the cudaFlow task callable. On the other hand, if a cudaFlow is not offloaded, the executor runs it once. For example, the following two versions represent the same execution logic.</p><pre class="m-code"><span class="c1">// version 1: explicitly offload a cudaFlow</span>
<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shm</span><span class="p">,</span> <span class="n">my_kernel</span><span class="p">,</span> <span class="n">my_kernel_args</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;my_kernel&quot;</span><span class="p">);</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>
<span class="p">};</span>

<span class="c1">// version 2 (same as version 1): executor offloads the cudaFlow</span>
<span class="n">taskflow</span><span class="p">.</span><span class="n">emplace</span><span class="p">(</span><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span><span class="o">&amp;</span> <span class="n">cf</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">shm</span><span class="p">,</span> <span class="n">my_kernel</span><span class="p">,</span> <span class="n">my_kernel_args</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;my_kernel&quot;</span><span class="p">);</span>
<span class="p">};</span></pre><p>We currently support the following methods to update task parameters from an offloaded cudaFlow:</p><ul><li><a href="classtf_1_1cudaFlow.html#abab3a11129e6286c1de3deecedae8090" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_kernel</a> updates the parameters of a kernel task</li><li><a href="classtf_1_1cudaFlow.html#a7972c77ba5f533b69e4b1dc55e87374d" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_copy</a> updates the parameters of a memcpy task to form a copy task</li><li><a href="classtf_1_1cudaFlow.html#af5f4cd1fc858a7725bbf57db629bdc34" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_memcpy</a> updates the parameters of a memcpy task</li><li><a href="classtf_1_1cudaFlow.html#a603072d44265de60647a7bcc5aaebace" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_memset</a> updates the parameters of a memset task</li><li><a href="classtf_1_1cudaFlow.html#a4a319c3e47fc538f6c31a7317c6a17e0" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_fill</a> updates the parameters of a memset task to form a fill task</li><li><a href="classtf_1_1cudaFlow.html#a62a042795e4a089ab633d809af6108a6" class="m-doc">tf::<wbr />cudaFlow::<wbr />update_zero</a> updates the parameters of a memset task to form a zero task</li></ul><p>Please visit the reference page of <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> for more details.</p><aside class="m-note m-warning"><h4>Attention</h4><p>There are quite a few limitations on update methods:</p><ul><li>kernel task<ul><li>The kernel function is not allowed to change</li></ul></li><li>memset and memcpy tasks:<ul><li>The CUDA device(s) to which the operand(s) was allocated/mapped cannot change</li><li>The source/destination memory must be allocated from the same contexts as the original source/destination memory.</li></ul></li></ul></aside></section><section id="C6_UsecudaFlowInAStandaloneEnvironment"><h2><a href="#C6_UsecudaFlowInAStandaloneEnvironment">Use cudaFlow in a Standalone Environment</a></h2><p>You can use <a href="classtf_1_1cudaFlow.html" class="m-doc">tf::<wbr />cudaFlow</a> in a standalone environment without going through <a href="classtf_1_1Taskflow.html" class="m-doc">tf::<wbr />Taskflow</a> and offloads it to GPU from the caller thread. All the features we have discussed so far are applicable for the standalone use.</p><pre class="m-code"><span class="n">tf</span><span class="o">::</span><span class="n">cudaFlow</span> <span class="n">cf</span><span class="p">;</span>  <span class="c1">// create a standalone cudaFlow</span>

<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dx</span><span class="p">,</span> <span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_x&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">h2d_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">dy</span><span class="p">,</span> <span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;h2d_y&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_x</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hx</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dx</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_x&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">d2h_y</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">hy</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span> <span class="n">dy</span><span class="p">,</span> <span class="n">N</span><span class="p">).</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;d2h_y&quot;</span><span class="p">);</span>
<span class="n">tf</span><span class="o">::</span><span class="n">cudaTask</span> <span class="n">saxpy</span> <span class="o">=</span> <span class="n">cf</span><span class="p">.</span><span class="n">kernel</span><span class="p">((</span><span class="n">N</span><span class="o">+</span><span class="mi">255</span><span class="p">)</span><span class="o">/</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">saxpy</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mf">2.0f</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">)</span>
                       <span class="p">.</span><span class="n">name</span><span class="p">(</span><span class="s">&quot;saxpy&quot;</span><span class="p">);</span>

<span class="n">saxpy</span><span class="p">.</span><span class="n">succeed</span><span class="p">(</span><span class="n">h2d_x</span><span class="p">,</span> <span class="n">h2d_y</span><span class="p">)</span>   <span class="c1">// kernel runs after  host-to-device copy</span>
     <span class="p">.</span><span class="n">precede</span><span class="p">(</span><span class="n">d2h_x</span><span class="p">,</span> <span class="n">d2h_y</span><span class="p">);</span>  <span class="c1">// kernel runs before device-to-host copy</span>

<span class="n">cf</span><span class="p">.</span><span class="n">offload</span><span class="p">();</span>  <span class="c1">// offload and run the standalone cudaFlow once</span></pre><aside class="m-note m-warning"><h4>Attention</h4><p>When using cudaFlow in a standalone environment, it is your choice and responsibility to decide its GPU context and ensure that GPU memory operations are correctly performed.</p></aside></section>
      </div>
    </div>
  </div>
</article></main>
<div class="m-doc-search" id="search">
  <a href="#!" onclick="return hideSearch()"></a>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-m-8 m-push-m-2">
        <div class="m-doc-search-header m-text m-small">
          <div><span class="m-label m-default">Tab</span> / <span class="m-label m-default">T</span> to search, <span class="m-label m-default">Esc</span> to close</div>
          <div id="search-symbolcount">&hellip;</div>
        </div>
        <div class="m-doc-search-content">
          <form>
            <input type="search" name="q" id="search-input" placeholder="Loading &hellip;" disabled="disabled" autofocus="autofocus" autocomplete="off" spellcheck="false" />
          </form>
          <noscript class="m-text m-danger m-text-center">Unlike everything else in the docs, the search functionality <em>requires</em> JavaScript.</noscript>
          <div id="search-help" class="m-text m-dim m-text-center">
            <p class="m-noindent">Search for symbols, directories, files, pages or
            modules. You can omit any prefix from the symbol or file path; adding a
            <code>:</code> or <code>/</code> suffix lists all members of given symbol or
            directory.</p>
            <p class="m-noindent">Use <span class="m-label m-dim">&darr;</span>
            / <span class="m-label m-dim">&uarr;</span> to navigate through the list,
            <span class="m-label m-dim">Enter</span> to go.
            <span class="m-label m-dim">Tab</span> autocompletes common prefix, you can
            copy a link to the result using <span class="m-label m-dim">⌘</span>
            <span class="m-label m-dim">L</span> while <span class="m-label m-dim">⌘</span>
            <span class="m-label m-dim">M</span> produces a Markdown link.</p>
          </div>
          <div id="search-notfound" class="m-text m-warning m-text-center">Sorry, nothing was found.</div>
          <ul id="search-results"></ul>
        </div>
      </div>
    </div>
  </div>
</div>
<script src="search-v1.js"></script>
<script src="searchdata-v1.js" async="async"></script>
<footer><nav>
  <div class="m-container">
    <div class="m-row">
      <div class="m-col-l-10 m-push-l-1">
        <p>Taskflow handbook is part of the <a href="https://taskflow.github.io">Taskflow project</a>, copyright © <a href="https://tsung-wei-huang.github.io/">Dr. Tsung-Wei Huang</a>, 2018&ndash;2020.<br />Generated by <a href="https://doxygen.org/">Doxygen</a> 1.8.20 and <a href="https://mcss.mosra.cz/">m.css</a>.</p>
      </div>
    </div>
  </div>
</nav></footer>
</body>
</html>
